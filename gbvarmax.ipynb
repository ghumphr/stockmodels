{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8631713,"sourceType":"datasetVersion","datasetId":5168426}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.utils import check_random_state\nfrom sklearn.metrics import mean_squared_error\n\n!pip install pmdarima\nfrom pmdarima import auto_arima\n\n# This will help to eliminate some irritating warnings\npd.options.mode.copy_on_write = True","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nclass GBVARMAX:\n    \"\"\"\n    Gradient Boosting Vector Autoregressive Moving Average with eXogeneous Variables\n\n    This class implements a Gradient Boosting algorithm for regression with \n    lagged dependent variables. It supports \n    multiple output variables and includes features such as:\n\n    - **Lagged dependent variables:** Incorporates past values of the target \n      variables as features.\n    - **Lagged prediction errors:** Uses past prediction errors as features.\n    - **Validation split:** Enables automatic splitting of data for model \n      evaluation during training.\n\n    Parameters\n    ----------\n    n_estimators : int, default=100\n        The number of boosting stages to perform.\n\n    learning_rate : float, default=0.01\n        Learning rate shrinks the contribution of each tree by `learning_rate`.\n\n    max_depth : int, default=5\n        Maximum depth of the individual regression trees.\n\n    n_lags : int, default=4\n        The number of lags of the target variable to include as features.\n\n    random_state : int or None, default=None\n        Controls the randomness of the estimator.\n\n    validation_split : float, default=0.2\n        The proportion of the training set to use for validation.\n\n    \"\"\"\n\n    def __init__(self, n_estimators=200, learning_rate=0.01, max_depth=4, \n                 n_lags=4, random_state=None, validation_split=0.2):\n        self.n_estimators = n_estimators\n        self.learning_rate = learning_rate\n        self.max_depth = max_depth\n        self.n_lags = n_lags\n        self.random_state = random_state\n        self.validation_split = validation_split\n        self.trees_ = []\n        self.val_scores_ = np.array([])   # List to store validation scores for each output\n        self.train_scores_ = np.array([]) # List to store training scores for each output\n\n    def _create_lagged_features(self, y, y_pred=None, x0=None, x1=None):\n        \"\"\"\n        Creates lagged features of the target variable and prediction errors using pandas.DataFrame.shift().\n\n        Args:\n          y: pandas.DataFrame of shape (n_samples, n_outputs)\n              Target values.\n          y_pred: array-like of shape (n_samples, n_outputs)\n              Predicted values. \n\n        Returns:\n          lagged_features: array-like \n              Array containing lagged values of y and prediction errors.\n        \"\"\"\n        lagged_df = y.copy()\n        for i in range(1, self.n_lags + 1):\n            for col in lagged_df.columns:\n                lagged_df[f'{col}_lag_{i}'] = lagged_df[col].shift(i)\n                lagged_df = lagged_df.copy()\n                \n        # Fill NaN values in lagged features with the mean of the corresponding column \n        for col in lagged_df.columns:\n            lagged_df[col] = lagged_df[col].fillna(lagged_df[col].mean()) \n            \n        lagged_features = lagged_df.values \n\n        if y_pred is None:\n            y_pred = np.tile(self.y_mean_, (y.shape[0], 1))\n        lagged_pred_df = pd.DataFrame(y_pred)\n        for i in range(1, self.n_lags + 1):\n            for col in lagged_pred_df.columns:\n                lagged_pred_df[f'{col}_pred_lag_{i}'] = lagged_pred_df[col].shift(i)\n                lagged_pred_df = lagged_pred_df.copy()\n        lagged_errors = lagged_df.values - lagged_pred_df.values \n            \n        # If we don't know what the error is, use 0\n        lagged_errors = np.nan_to_num(lagged_errors, nan=0)\n            \n        lagged_features = np.concatenate((lagged_features, lagged_errors), axis=1)\n        return pd.DataFrame(lagged_features)\n\n    def fit(self, y, x=None, x_d=None):\n        \"\"\"\n        Fit the Gradient Boosting VARMAX model.\n\n        Parameters\n        ----------\n        y : pandas.DataFrame of shape (n_samples, n_outputs)\n            Training target values, where n_outputs is the number of output variables. \n\n        x : pandas.Dataframe of shape (n_samples, n_exog)\n            Exogenous predictors\n\n        x_d : dict of pandas.Dataframe of shape (n_samples, n_exog_i)\n            Exogeneous predictors for specific y_i\n\n        Returns\n        -------\n        self : object\n            Fitted estimator.\n        \"\"\"\n        if not isinstance(y, pd.DataFrame):\n            raise ValueError(\"Input 'y' must be a pandas DataFrame.\")\n\n        n_samples_train = y.shape[0]\n        n_outputs = y.shape[1] \n\n        # Generate random indices for train/validation split\n        indices = np.arange(n_samples_train)\n        rng = check_random_state(self.random_state)\n        rng.shuffle(indices)\n        train_size = int((1 - self.validation_split) * n_samples_train)\n        train_indices = indices[:train_size]\n        val_indices = indices[train_size:]\n\n        # Calculate mean on training data only\n        self.y_mean_ = np.mean(y.iloc[train_indices], axis=0) \n        y_pred = y.copy() \n        y_pred.iloc[:] = self.y_mean_ \n\n\n        self.val_scores_ = np.zeros((self.n_estimators, n_outputs))\n        self.train_scores_ = np.zeros((self.n_estimators, n_outputs))\n        for i in range(self.n_estimators):\n\n            # Create lagged features for the training data \n            lagged_features_all = self._create_lagged_features(y, y_pred)\n            lagged_features_train = lagged_features_all.iloc[train_indices].copy()\n\n            # Fit a decision tree for each output variable\n            self.trees_.append([])\n#            print(\">>>\")\n#            print(lagged_features_all.head(2))\n#            print(lagged_features_all.tail(2))\n            \n            for j in range(n_outputs):\n                tree = DecisionTreeRegressor(max_depth=self.max_depth)\n                tree.fit(lagged_features_train, y.iloc[train_indices, j]) \n                self.trees_[-1].append(tree)\n\n            # Update predictions for all data\n            lagged_features_all = self._create_lagged_features(y_pred)\n            for j in range(n_outputs):\n                y_pred.iloc[:, j] -= self.learning_rate * self.trees_[-1][j].predict(lagged_features_all) \n\n            # Calculate validation error \n            y_pred_val = y_pred.iloc[val_indices]\n            y_val = y.iloc[val_indices]\n            for j in range(n_outputs):\n                self.val_scores_[i, j] = mean_squared_error(y_val.iloc[:, j], y_pred_val.iloc[:,j])\n\n            # Calculate training error\n            y_pred_train = y_pred.iloc[train_indices]\n            y_train = y.iloc[train_indices]\n            for j in range(n_outputs):\n                self.train_scores_[i, j] = mean_squared_error(y_train.iloc[:, j], y_pred_train.iloc[:,j])\n\n\n\n            print(\"i = \", i, \" of \", self.n_estimators)\n\n        return self\n\n    def forecast(self, y):\n        \"\"\"\n        Forecast future values for y.\n\n        Parameters\n        ----------\n        y : pandas.DataFrame of shape (n_samples, n_outputs)\n            Historical target values for forecasting. \n\n        Returns\n        -------\n        y_forecast : array-like of shape (n_samples, n_outputs)\n            Forecasted values for y.\n        \"\"\"\n        n_outputs = len(self.trees_[0])\n        y_forecast = np.full_like(y, self.y_mean_) \n\n        if not isinstance(y, pd.DataFrame):\n            raise ValueError(\"Input 'y' must be a pandas DataFrame.\")\n\n        for i in range(self.n_lags, len(y)):\n            lagged_features = self._create_lagged_features(y[:i], y_forecast[:i]) \n            for j in range(n_outputs):\n                y_forecast[i, j] = self.y_mean_[j] + sum(tree.predict(lagged_features) for tree in self.trees_[:, j])\n\n        return y_forecast","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tb = pd.read_csv(\"/kaggle/input/sharadar-etfs/sharadar/SFP.csv/SHARADAR_SFP_2_fb4f5d2244276f3cfeca03f46b122d99.csv\")\n\n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# This code isn't pretty, but it is thousands of times faster than the \n# easy-to-read alternative that calculates log return by ticker\ntb = tb.sort_values(by=[\"ticker\", \"date\"])\ntb[\"revenue\"] = tb[\"close\"] * tb[\"volume\"]\ntb[\"return\"] = tb[\"closeadj\"] / tb[\"closeadj\"].shift()\ntb[\"tickeryesterday\"] = tb[\"ticker\"].shift()\ntb[\"logreturn\"] = np.log(tb[\"closeadj\"] / tb[\"closeadj\"].shift())\ntb.loc[tb[\"ticker\"] != tb[\"tickeryesterday\"], \"return\"] = np.nan\ntb.drop(\"tickeryesterday\", axis=1, inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tb.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def tickers_by_revenue(ltb):\n    stickers = ltb.groupby(\"ticker\")[\"revenue\"].sum().sort_values(ascending=False).index.to_list()\n    return stickers\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"n_base_etfs = 200\nn_model_etfs = 16\n\nwindow_size = 1250\nskip_size = 20\n\nfirst_train_data_date = \"2002-01-01\"\nfirst_val_data_date = \"2019-01-01\"\nfirst_test_data_date = \"2023-01-01\"\nfirst_eval_data_date = \"2024-01-01\"\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def gen_ts_vectors(stb, tickers):\n    vs = []\n    for t in tickers:\n        v = stb.loc[stb[\"ticker\"] == t, [\"date\", \"logreturn\"]]\n        v = v.set_index(\"date\", drop=True)\n        vs.append(v)\n    \n    rv = pd.concat(vs, axis=1)\n    rv.columns = tickers\n    return rv\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tb_train = tb[(tb[\"date\"] >= first_train_data_date) & (tb[\"date\"] < first_val_data_date)]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_tickers = tickers_by_revenue(tb_train)[0:20]\n\ntrain_endog = gen_ts_vectors(tb_train, train_tickers)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_endog.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_endog[np.isnan(train_endog)] = 0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gbv = GBVARMAX()\ngbv.fit(train_endog)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-11T00:42:52.114Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"xx = 0\npd.DataFrame(gbv.val_scores_).iloc[:,xx].plot(title=\"Error For \" + train_endog.columns[xx] + \" Log Returns\", xlabel=\"# of Trees\", ylabel=\"MSE\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-11T00:42:52.114Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}